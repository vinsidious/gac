# gac Configuration Example

# REQUIRED - Fully qualified model in format 'provider:model'
# GAC_MODEL=groq:meta-llama/llama-4-scout-17b-16e-instruct
# GAC_MODEL=minimax:abab6.5s-chat
# GAC_MODEL=together:meta-llama/Llama-3.2-3B-Instruct-Turbo

# AI Provider API Keys
# Uncomment and add your key for the provider(s) you want to use
# ANTHROPIC_API_KEY=your_key_here
# CEREBRAS_API_KEY=your_key_here
# GEMINI_API_KEY=your_key_here
# GROQ_API_KEY=your_key_here
# LMSTUDIO_API_KEY=your_key_here  # Optional - only if your LM Studio instance requires authentication
# MINIMAX_API_KEY=your_key_here
# OLLAMA_API_KEY=your_key_here  # Optional - only if your Ollama instance requires authentication
# OPENAI_API_KEY=your_key_here
# TOGETHER_API_KEY=your_key_here
# OPENROUTER_API_KEY=your_key_here
# SYNTHETIC_API_KEY=your_key_here
# ZAI_API_KEY=your_key_here

# AI Provider URLs
# Uncomment and customize if you're using custom endpoints
# LMSTUDIO_API_URL=http://localhost:1234  # LM Studio server URL
# OLLAMA_API_URL=http://localhost:11434  # Ollama server URL


# OPTIONAL gac Settings
# GAC_WARNING_LIMIT_TOKENS=4096
# GAC_MAX_OUTPUT_TOKENS=512
# GAC_TEMPERATURE=0.7
# GAC_ALWAYS_INCLUDE_SCOPE=true
# GAC_VERBOSE=true  # Generate detailed commit messages with motivation, architecture, and impact sections
# GAC_ZAI_USE_CODING_PLAN=false  # Set to true to use coding API endpoint instead of regular API
